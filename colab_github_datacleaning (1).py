# -*- coding: utf-8 -*-
"""Colab_Github_DataCleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HzDEbbYCRJ-Qkwvt9M_xlcXkfq_JuLKc

# ðŸ“Œ Colab: Load Dataset dari GitHub + Data Cleaning (Missing Value)

Notebook ini dibuat untuk:
- Mengambil dataset **langsung dari GitHub** (bukan Google Drive)
- Mendukung file **CSV / Excel**
- Melakukan **Pra Pemrosesan Data (Data Cleaning)** agar data bersih dan bebas *missing value*

> âœ… Cara pakai: ganti nilai `GITHUB_FILE_URL` pada Cell 3 dengan link file dataset kamu di GitHub (boleh link `github.com/.../blob/...` atau link `raw.githubusercontent.com/...`).

---

## 1) Install & Import Library
Colab biasanya sudah punya `pandas` dan `numpy`. Jika kamu pakai Excel (`.xlsx`), pastikan `openpyxl` tersedia.
"""

# Jika perlu (khusus Excel), jalankan:
# !pip -q install openpyxl

import re
import os
import numpy as np
import pandas as pd
from IPython.display import display
from urllib.parse import urlparse

"""## 2) Load Dataset dari GitHub (Bukan Drive)

âœ… Kamu bisa pakai:
- Link GitHub biasa: `https://github.com/user/repo/blob/main/data.csv`
- Link RAW: `https://raw.githubusercontent.com/user/repo/main/data.csv`

"""

# =========================
# SET LINK DATASET GITHUB DI SINI
# =========================
GITHUB_FILE_URL = "https://raw.githubusercontent.com/nanocassander-lab/Bekasmotorku/refs/heads/main/motor_second.csv"  # <-- ganti ini

def github_to_raw(url: str) -> str:
    """Ubah link GitHub file (blob) menjadi raw.githubusercontent.com"""
    url = url.strip()
    if "raw.githubusercontent.com" in url:
        return url

    # Pola: https://github.com/<user>/<repo>/blob/<branch>/<path>
    m = re.match(r"^https?://github\.com/([^/]+)/([^/]+)/blob/([^/]+)/(.+)$", url)
    if m:
        user, repo, branch, path = m.groups()
        return f"https://raw.githubusercontent.com/{user}/{repo}/{branch}/{path}"

    return url  # kalau bukan format itu, kembalikan apa adanya

def get_extension(url: str) -> str:
    """Ambil ekstensi file dari URL (csv/xlsx/xls)"""
    # buang query string
    path = urlparse(url).path
    if "." in path:
        return path.split(".")[-1].lower()
    return ""

def load_from_github(url: str) -> pd.DataFrame:
    raw_url = github_to_raw(url)
    ext = get_extension(raw_url)

    print("RAW URL:", raw_url)

    if ext == "csv":
        # kalau delimiter dataset kamu bukan koma, tambahkan: sep=';'
        return pd.read_csv(raw_url)
    elif ext in ("xlsx", "xls"):
        # kadang read_excel dari URL gagal karena redirect/limit,
        # maka kita coba langsung, kalau gagal download dulu.
        try:
            return pd.read_excel(raw_url)
        except Exception as e:
            print("read_excel dari URL gagal, mencoba download manual...\nError:", e)
            import requests
            filename = f"dataset.{ext}"
            r = requests.get(raw_url)
            r.raise_for_status()
            with open(filename, "wb") as f:
                f.write(r.content)
            return pd.read_excel(filename)
    else:
        raise ValueError("Format file tidak dikenali. Pastikan URL berakhiran .csv / .xlsx / .xls")

# Load dataset
df = load_from_github(GITHUB_FILE_URL)

print("âœ… Data berhasil dimuat!")
print("Shape:", df.shape)
display(df.head())

"""## 3) Cek Struktur Data Awal
Cell ini menampilkan ringkasan kolom, tipe data, serta jumlah missing value awal.

"""

print("Kolom:")
print(df.columns.tolist())

print("\nInfo Dataset:")
df.info()

print("\nJumlah Missing Value per Kolom (awal):")
display(df.isna().sum().sort_values(ascending=False))

"""## 4) Pra Pemrosesan Data (Data Cleaning)

Tahap pra pemrosesan dilakukan untuk memastikan data yang digunakan bersih dan bebas dari nilai kosong (missing value).

Langkah yang dilakukan:
1. Menyamakan format nilai kosong (mis. string kosong, `NA`, `null`, `-`) menjadi `NaN`.
2. Menghapus data duplikat.
3. Mengecek jumlah missing value per kolom.
4. Menangani missing value (imputasi):
   - Kolom numerik diisi dengan **median**
   - Kolom kategorikal/teks diisi dengan **modus**
5. Validasi akhir: memastikan tidak ada missing value tersisa.

"""

def basic_cleaning(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # 1) Rapikan nama kolom
    df.columns = df.columns.str.strip()

    # 2) Samakan berbagai bentuk 'kosong' menjadi NaN
    df = df.replace(r'^\s*$', np.nan, regex=True)  # string kosong/spasi -> NaN
    missing_placeholders = ['-', '--', 'NA', 'N/A', 'na', 'n/a', 'null', 'NULL', 'None', 'none', '?']
    df = df.replace(missing_placeholders, np.nan)

    # 3) Hapus duplikat
    before = len(df)
    df = df.drop_duplicates()
    print(f"Duplikat dihapus: {before - len(df)} baris")

    # 4) Laporan missing value (sebelum imputasi)
    missing_count = df.isna().sum()
    missing_count = missing_count[missing_count > 0].sort_values(ascending=False)

    if len(missing_count) == 0:
        print("Tidak ada missing value terdeteksi âœ…")
    else:
        missing_pct = (missing_count / len(df) * 100).round(2)
        report = pd.DataFrame({"missing": missing_count, "missing_%": missing_pct})
        print("Missing value per kolom (sebelum cleaning):")
        display(report)

    # 5) Rapikan spasi untuk kolom teks
    obj_cols = df.select_dtypes(include="object").columns
    for col in obj_cols:
        df[col] = df[col].astype(str).str.strip()
        # akibat astype(str), NaN bisa jadi string -> kembalikan jadi NaN
        df.loc[df[col].isin(["nan", "NaN", "None"]), col] = np.nan

    # 6) Imputasi missing value
    num_cols = df.select_dtypes(include=np.number).columns.tolist()
    cat_cols = [c for c in df.columns if c not in num_cols]

    # Numerik -> median
    if len(num_cols) > 0:
        df[num_cols] = df[num_cols].fillna(df[num_cols].median(numeric_only=True))

    # Kategorikal/teks -> modus (most frequent)
    for col in cat_cols:
        if df[col].isna().any():
            mode = df[col].mode(dropna=True)
            fill_value = mode.iloc[0] if len(mode) > 0 else "Unknown"
            df[col] = df[col].fillna(fill_value)

    # 7) Validasi akhir
    total_missing = int(df.isna().sum().sum())
    print("Total missing value setelah cleaning:", total_missing)
    assert total_missing == 0, "Masih ada missing value, cek ulang kolom tertentu."

    return df

df_clean = basic_cleaning(df)

print("âœ… Data cleaning selesai. Shape akhir:", df_clean.shape)
display(df_clean.head())

"""## 5) Simpan Hasil Cleaning (Opsional)

- File hasil cleaning akan disimpan sebagai `data_bersih.csv`
- Kamu juga bisa langsung download dari Colab.

"""

df_clean.to_csv("data_bersih.csv", index=False)
print("âœ… Tersimpan: data_bersih.csv")

# Download ke komputer (aktifkan jika perlu):
from google.colab import files
files.download("data_bersih.csv")

"""## 6) (Opsional) Lanjut Modeling Otomatis
Jika kamu ingin lanjut ke machine learning, isi `TARGET_COL` dengan nama kolom target/label.
Notebook akan mencoba mendeteksi apakah ini **klasifikasi** atau **regresi**.

"""

# =========================
# OPSIONAL: MODELING
# =========================
# Jika tidak ingin modeling, abaikan cell ini.

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

print("Kolom tersedia:")
print(df_clean.columns.tolist())

TARGET_COL = ""  # <-- isi misalnya: "label" atau "harga"

if TARGET_COL == "":
    print("âš ï¸ TARGET_COL masih kosong. Isi TARGET_COL jika ingin training model.")
else:
    X = df_clean.drop(columns=[TARGET_COL])
    y = df_clean[TARGET_COL]

    is_numeric_target = pd.api.types.is_numeric_dtype(y)
    n_unique = y.nunique()
    task = "regression" if (is_numeric_target and n_unique > 20) else "classification"
    print("Task terdeteksi:", task)

    num_features = X.select_dtypes(include=np.number).columns.tolist()
    cat_features = [c for c in X.columns if c not in num_features]

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocess = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_features),
            ("cat", categorical_transformer, cat_features),
        ],
        remainder="drop"
    )

    model = RandomForestRegressor(n_estimators=300, random_state=42) if task == "regression" \
            else RandomForestClassifier(n_estimators=300, random_state=42)

    pipe = Pipeline(steps=[("preprocess", preprocess),
                          ("model", model)])

    if task == "classification":
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    else:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    if task == "classification":
        acc = accuracy_score(y_test, y_pred)
        print("Accuracy:", acc)
        print("\nClassification Report:\n")
        print(classification_report(y_test, y_pred))
    else:
        mae = mean_absolute_error(y_test, y_pred)
        rmse = mean_squared_error(y_test, y_pred, squared=False)
        r2 = r2_score(y_test, y_pred)
        print("MAE :", mae)
        print("RMSE:", rmse)
        print("R2  :", r2)